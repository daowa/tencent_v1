{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 读入基础数据\n",
    "## 确定是读入训练集进行训练，还是读入测试集生成提交结果\n",
    "### 仅训练集\n",
    "rawData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "### 训练集和测试集。注意这里把训练集也读入了，为了dummy的时候划分一致。后续需要删除训练集数据\n",
    "# rawData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "# rawData.drop(['conversionTime'],axis=1,inplace=True) #先去除conversionTime\n",
    "# rawData.insert(0, 'instanceID', pd.Series(np.zeros(rawData['label'].count())))#插入instanceID使append不会乱序\n",
    "# rawData = pd.concat([rawData,pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\test.csv')])\n",
    "## 将部分数据读入\n",
    "rawData['date'] = rawData['clickTime'].apply(lambda x: int(x/10000))\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\user.csv'), on='userID')\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\ad.csv'), on='creativeID')\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\position.csv'), on='positionID')\n",
    "### 清除29、30两天的数据(目前发现效果并不好)\n",
    "# rawData = rawData[((rawData['date']==29)|(rawData['date']==30)) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'label', u'clickTime', u'conversionTime', u'creativeID', u'userID',\n",
       "       u'positionID', u'connectionType', u'telecomsOperator', u'date', u'age',\n",
       "       u'gender', u'education', u'marriageStatus', u'haveBaby', u'hometown',\n",
       "       u'residence', u'adID', u'camgaignID', u'advertiserID', u'appID',\n",
       "       u'appPlatform', u'sitesetID', u'positionType'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对点击时间进行处理\n",
    "def dealTime(clicktime):\n",
    "    hour = int(str(clicktime)[2:4])\n",
    "    minute = int(str(clicktime)[4:6])\n",
    "    x = int(60*hour+minute)/30\n",
    "    if x in [18,19,20]:\n",
    "        return 1\n",
    "    elif x in [21,22,23]:\n",
    "        return 2\n",
    "    elif x in [4,24]:\n",
    "        return 3\n",
    "    elif x in [25,26]:\n",
    "        return 4\n",
    "    elif x in [2,3,6,10,27,31,47]:\n",
    "        return 5\n",
    "    elif x in [0,1,5,7,8,9,11,28,30,44,45]:\n",
    "        return 6\n",
    "    elif x in [13,14]:\n",
    "        return 8\n",
    "    elif x in [15]:\n",
    "        return 9\n",
    "    else: #中下的最多，所以else来处理\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读入更多特征\n",
    "## 读入age\n",
    "rawData.drop(['age'], axis=1, inplace=True) #有些原有字段中有的但经过处理的，先删除，再读入\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\age.csv'), on='userID')\n",
    "## residence提取省份\n",
    "# rawData['residence'] = rawData['residence'].apply(lambda x: int(str(x)[-4:-2]) if x>0 else 0)\n",
    "## 读入appCategory\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\appCategory.csv'), on='creativeID')\n",
    "## 读入clickTime_hour\n",
    "rawData['clickTime_hour'] = rawData['clickTime'].apply(dealTime)\n",
    "## 读入lastNCVR_advertiserID\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_advertiserID.csv'), on=['date', 'creativeID'])\n",
    "## 读入lastNCVR_appID\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_appID.csv'), on=['date', 'creativeID'])\n",
    "## 读入lastNCVR_positionType\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_positionType.csv'), on=['date', 'positionID'])\n",
    "## 读入lastNCVR_sitesetID\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_sitesetID.csv'), on=['date', 'positionID'])\n",
    "## 读入lastNCVR_camgaignID\n",
    "# rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_camgaignID.csv'), on=['date', 'creativeID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 交叉特征\n",
    "# rawData['age*positionType'] = rawData['age'] + 100*rawData['positionType']\n",
    "# rawData['gender*positionType'] = rawData['gender'] + 100*rawData['positionType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'clickTime',\n",
       " 'conversionTime',\n",
       " 'creativeID',\n",
       " 'userID',\n",
       " 'positionID',\n",
       " 'connectionType',\n",
       " 'telecomsOperator',\n",
       " 'date',\n",
       " 'gender',\n",
       " 'education',\n",
       " 'marriageStatus',\n",
       " 'haveBaby',\n",
       " 'hometown',\n",
       " 'residence',\n",
       " 'adID',\n",
       " 'camgaignID',\n",
       " 'advertiserID',\n",
       " 'appID',\n",
       " 'appPlatform',\n",
       " 'sitesetID',\n",
       " 'positionType',\n",
       " 'age',\n",
       " 'appCategory',\n",
       " 'clickTime_hour',\n",
       " 'lastNCVR_advertiserID',\n",
       " 'lastNCVR_appID',\n",
       " 'lastNCVR_positionType',\n",
       " 'lastNCVR_sitesetID']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rawData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 特征哑变量化与离散化\n",
    "import sklearn.preprocessing as preprocessing\n",
    "## 离散变量哑变量化\n",
    "def featureDummy(features):\n",
    "    for feature in features:\n",
    "        global rawData #声明data是global而不是局部变量\n",
    "        dummies = pd.get_dummies(rawData[feature], prefix=feature)\n",
    "        rawData = pd.concat([rawData, dummies], axis=1)\n",
    "        rawData.drop([feature], axis=1, inplace=True)\n",
    "## 连续变量标准化\n",
    "dataScaleTrain = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv') #用train的数据来求scale\n",
    "dataScaleTrain['date'] = dataScaleTrain['clickTime'].apply(lambda x: int(x/10000))\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_advertiserID.csv'), on=['date', 'creativeID'])\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_appID.csv'), on=['date', 'creativeID'])\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_positionType.csv'), on=['date', 'positionID'])\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_sitesetID.csv'), on=['date', 'positionID'])\n",
    "# dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_camgaignID.csv'), on=['date', 'creativeID'])\n",
    "def featureScale(features):\n",
    "    for feature in features:\n",
    "        global rawData #声明data是global而不是局部变量\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaleParam = scaler.fit(dataScaleTrain[feature])\n",
    "        rawData[feature] = scaler.fit_transform(rawData[feature], scaleParam)\n",
    "# 哑变量化与离散化\n",
    "featureDummy(['connectionType', 'telecomsOperator', 'marriageStatus','haveBaby', 'appPlatform', \n",
    "              'education', 'sitesetID', 'appCategory', 'clickTime_hour','appID','age', 'gender','positionType'])\n",
    "featureScale(['lastNCVR_appID', 'lastNCVR_advertiserID','lastNCVR_positionType', 'lastNCVR_sitesetID'])\n",
    "# 删除不再使用的数据，节省内存\n",
    "del dataScaleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3749528L,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData['lastNCVR_appID'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 提取特征\n",
    "## 筛掉一些字段\n",
    "data = rawData.drop(['clickTime', 'conversionTime', 'creativeID', 'userID', 'positionID', 'date', \n",
    "                     'hometown', 'advertiserID', 'adID', 'camgaignID','residence'], axis=1)\n",
    "# data = rawData.drop(['clickTime', 'creativeID', 'userID', 'positionID', 'date', \n",
    "#                      'hometown', 'residence','advertiserID', 'adID', 'camgaignID'], axis=1)\n",
    "# data = data.filter(regex='label|connectionType_*|telecomsOperato_*|appPlatform_*|lastNCVR_appID_+|lastNCVR_advertiserID_+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'lastNCVR_advertiserID', 'lastNCVR_appID', 'lastNCVR_positionType', 'lastNCVR_sitesetID', 'connectionType_0', 'connectionType_1', 'connectionType_2', 'connectionType_3', 'connectionType_4', 'telecomsOperator_0', 'telecomsOperator_1', 'telecomsOperator_2', 'telecomsOperator_3', 'marriageStatus_0', 'marriageStatus_1', 'marriageStatus_2', 'marriageStatus_3', 'haveBaby_0', 'haveBaby_1', 'haveBaby_2', 'haveBaby_3', 'haveBaby_4', 'haveBaby_5', 'haveBaby_6', 'appPlatform_1', 'appPlatform_2', 'education_0', 'education_1', 'education_2', 'education_3', 'education_4', 'education_5', 'education_6', 'education_7', 'sitesetID_0', 'sitesetID_1', 'sitesetID_2', 'appCategory_0', 'appCategory_2', 'appCategory_101', 'appCategory_104', 'appCategory_106', 'appCategory_108', 'appCategory_201', 'appCategory_203', 'appCategory_209', 'appCategory_301', 'appCategory_402', 'appCategory_407', 'appCategory_408', 'appCategory_503', 'clickTime_hour_1', 'clickTime_hour_2', 'clickTime_hour_3', 'clickTime_hour_4', 'clickTime_hour_5', 'clickTime_hour_6', 'clickTime_hour_7', 'clickTime_hour_8', 'clickTime_hour_9', 'appID_14', 'appID_25', 'appID_68', 'appID_75', 'appID_83', 'appID_84', 'appID_88', 'appID_100', 'appID_105', 'appID_109', 'appID_113', 'appID_116', 'appID_123', 'appID_127', 'appID_137', 'appID_146', 'appID_150', 'appID_160', 'appID_195', 'appID_198', 'appID_205', 'appID_206', 'appID_218', 'appID_229', 'appID_262', 'appID_271', 'appID_278', 'appID_283', 'appID_284', 'appID_286', 'appID_293', 'appID_304', 'appID_319', 'appID_327', 'appID_328', 'appID_336', 'appID_350', 'appID_356', 'appID_360', 'appID_383', 'appID_389', 'appID_391', 'appID_419', 'appID_420', 'appID_421', 'appID_428', 'appID_434', 'appID_442', 'appID_465', 'appID_472', 'age_0', 'age_1', 'age_8', 'age_9', 'age_10', 'age_11', 'age_12', 'age_13', 'age_14', 'age_15', 'age_16', 'age_17', 'age_18', 'age_19', 'age_20', 'age_21', 'age_22', 'age_24', 'age_26', 'age_28', 'age_30', 'age_32', 'age_34', 'age_36', 'age_38', 'age_40', 'age_41', 'age_48', 'age_54', 'gender_0', 'gender_1', 'gender_2', 'positionType_0', 'positionType_1', 'positionType_2', 'positionType_3', 'positionType_4', 'positionType_5']\n"
     ]
    }
   ],
   "source": [
    "print list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "print len(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 平衡正反例\n",
    "def balance(train_pro):\n",
    "    ## 从正例中随机取出比例为a的数据，划分\n",
    "    train_size_p = int(data[data['label']==1]['label'].count()*train_pro)\n",
    "    train_size_n = int(train_size_p * 39.20424181) #原始样本中负例是正例的这个倍数(所有样本)\n",
    "#     train_size_n = int(train_size_p * 38.16572737) #原始样本中负例是正例的这个倍数（剔除了29和30号的样本）\n",
    "    sampler1 = np.random.permutation(data[data['label']==1]['label'].count())\n",
    "    index1 = data[data['label']==1].index[sampler1] #重排序后的索引\n",
    "    train1 = data[data['label']==1].ix[index1[:train_size_p]]\n",
    "    test1 = data[data['label']==1].ix[index1[train_size_p:]]\n",
    "    ## 从负例中随机取出相同数量的数据，划分\n",
    "    sampler0 = np.random.permutation(data[data['label']==0]['label'].count())\n",
    "    index0 = data[data['label']==0].index[sampler0] #重排序后的索引\n",
    "    train0 = data[data['label']==0].ix[index0[:train_size_n]]\n",
    "    test0 = data[data['label']==0].ix[index0[train_size_n:]]\n",
    "    ## 构建训练集和测试集\n",
    "    data_train = pd.concat([train1, train0])\n",
    "    data_test = pd.concat([test1, test0])\n",
    "    return data_train.as_matrix()[:,1:], data_test.as_matrix()[:,1:], data_train.as_matrix()[:,0],  data_test.as_matrix()[:,0]\n",
    "# 获取正反例\n",
    "trainX, testX, trainY, testY = balance(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "# X = all_data.as_matrix()[:,1:]\n",
    "# y = all_data.as_matrix()[:,0]\n",
    "# trainX, testX, trainY, testY = cross_validation.train_test_split(X, y, test_size=0.2, random_state=174)\n",
    "# y\n",
    "# 训练\n",
    "clf = linear_model.LogisticRegressionCV(Cs=10, penalty='l2', tol=1e-4, n_jobs=-1, cv=5)\n",
    "# clf = RandomForestClassifier()\n",
    "clf.fit(trainX, trainY)\n",
    "print clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P&R\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      1.00      0.99    731277\n",
      "        1.0       0.67      0.00      0.00     18653\n",
      "\n",
      "avg / total       0.97      0.98      0.96    749930\n",
      "\n",
      "loss:\n",
      "0.108385315363\n"
     ]
    }
   ],
   "source": [
    "# 评估模型结果\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "from sklearn import metrics\n",
    "predict = clf.predict(testX)\n",
    "print 'P&R'\n",
    "print metrics.classification_report(testY, predict)\n",
    "print 'loss:'\n",
    "predict_pro = clf.predict_proba(testX)[:, 1]\n",
    "predict_pro = predict_pro - predict_pro.min()\n",
    "print logloss(testY, predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03400536,  0.0159104 ,  0.01982378,  0.01971964,  0.05060371,\n",
       "        0.12946618,  0.20653763,  0.03281828,  0.02073877,  0.02703447])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict_pro.sum()/len(predict_pro)\n",
    "predict_pro[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf = pd.DataFrame({'columns':data.columns[1:], 'coef':list(clf.coef_.T)})\n",
    "cf.sort_values(by=['coef'],ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\coef_6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存分类器\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\6.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取分类器\n",
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 提取特征并预测\n",
    "## 只选择instanceID>0的记录，并重排序instanceID\n",
    "dataPredict = data[data.instanceID>0].sort_values(by='instanceID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPredict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataPredict.as_matrix()[:,2:]\n",
    "y = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'instanceID':range(1,dataPredict['instanceID'].count()+1),'prob':y[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
