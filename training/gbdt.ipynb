{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 读入训练和测试数据\n",
    "# ## 本地，同时平衡正反例\n",
    "# def balance(train_pro):\n",
    "#     ## 从正例中随机取出比例为a的数据，划分\n",
    "#     data = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "#     data['date'] = data['clickTime'].apply(lambda x: int(x/10000))\n",
    "#     data = data[data['date']<30] #去掉30号那天的数据\n",
    "#     train_size_p = int(data[data['label']==1]['label'].count()*train_pro)\n",
    "#     train_size_n = int(train_size_p * 38.2751) #原始样本中负例是正例的这个倍数(所有样本)\n",
    "#     sampler1 = np.random.permutation(data[data['label']==1]['label'].count())\n",
    "#     index1 = data[data['label']==1].index[sampler1] #重排序后的索引\n",
    "#     train1 = data[data['label']==1].ix[index1[:train_size_p]]\n",
    "#     test1 = data[data['label']==1].ix[index1[train_size_p:]]\n",
    "#     ## 从负例中随机取出相同数量的数据，划分\n",
    "#     sampler0 = np.random.permutation(data[data['label']==0]['label'].count())\n",
    "#     index0 = data[data['label']==0].index[sampler0] #重排序后的索引\n",
    "#     train0 = data[data['label']==0].ix[index0[:train_size_n]]\n",
    "#     test0 = data[data['label']==0].ix[index0[train_size_n:]]\n",
    "#     ## 构建训练集和测试集\n",
    "#     data_train = pd.concat([train1, train0])\n",
    "#     data_test = pd.concat([test1, test0])\n",
    "#     return data_train, data_test\n",
    "# # 获取正反例\n",
    "# trainData, testData = balance(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "trainData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "testData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instanceID</th>\n",
       "      <th>label</th>\n",
       "      <th>clickTime</th>\n",
       "      <th>creativeID</th>\n",
       "      <th>userID</th>\n",
       "      <th>positionID</th>\n",
       "      <th>connectionType</th>\n",
       "      <th>telecomsOperator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>310000</td>\n",
       "      <td>3745</td>\n",
       "      <td>1164848</td>\n",
       "      <td>3451</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>310000</td>\n",
       "      <td>2284</td>\n",
       "      <td>2127247</td>\n",
       "      <td>1613</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>310000</td>\n",
       "      <td>1456</td>\n",
       "      <td>2769125</td>\n",
       "      <td>5510</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>310000</td>\n",
       "      <td>4565</td>\n",
       "      <td>9762</td>\n",
       "      <td>4113</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>310000</td>\n",
       "      <td>49</td>\n",
       "      <td>2513636</td>\n",
       "      <td>3615</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instanceID  label  clickTime  creativeID   userID  positionID  \\\n",
       "0           1     -1     310000        3745  1164848        3451   \n",
       "1           2     -1     310000        2284  2127247        1613   \n",
       "2           3     -1     310000        1456  2769125        5510   \n",
       "3           4     -1     310000        4565     9762        4113   \n",
       "4           5     -1     310000          49  2513636        3615   \n",
       "\n",
       "   connectionType  telecomsOperator  \n",
       "0               1                 3  \n",
       "1               1                 3  \n",
       "2               2                 1  \n",
       "3               2                 3  \n",
       "4               1                 3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入基础数据\n",
    "for i,dt in enumerate([trainData,testData]):\n",
    "    dt['date'] = dt['clickTime'].apply(lambda x: int(x/10000))\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\user.csv'), on='userID')\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\ad.csv'), on='creativeID')\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\position.csv'), on='positionID')\n",
    "    if i == 0:\n",
    "        trainData = dt\n",
    "    elif i == 1:\n",
    "        testData = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = trainData[trainData['date']==28]\n",
    "trainData = trainData[trainData['date']<28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 对点击时间进行处理\n",
    "# def dealTime(clicktime):\n",
    "#     hour = int(str(clicktime)[2:4])\n",
    "#     minute = int(str(clicktime)[4:6])\n",
    "#     x = int(60*hour+minute)/30\n",
    "#     if x in [18,19,20]:\n",
    "#         return 1\n",
    "#     elif x in [21,22,23]:\n",
    "#         return 2\n",
    "#     elif x in [4,24]:\n",
    "#         return 3\n",
    "#     elif x in [25,26]:\n",
    "#         return 4\n",
    "#     elif x in [2,3,6,10,27,31,47]:\n",
    "#         return 5\n",
    "#     elif x in [0,1,5,7,8,9,11,28,30,44,45]:\n",
    "#         return 6\n",
    "#     elif x in [13,14]:\n",
    "#         return 8\n",
    "#     elif x in [15]:\n",
    "#         return 9\n",
    "#     else: #中下的最多，所以else来处理\n",
    "#         return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 读入更多特征\n",
    "# for i,dt in enumerate([trainData,testData]):\n",
    "#     ## 读入age\n",
    "#     dt.drop(['age'], axis=1, inplace=True) #有些原有字段中有的但经过处理的，先删除，再读入\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\age.csv'), on='userID')\n",
    "#     ## residence提取省份\n",
    "# #     dt['residence'] = dt['residence'].apply(lambda x: int(str(x)[-4:-2]) if x>0 else 0)\n",
    "#     ## 读入appCategory\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\appCategory.csv'), on='creativeID')\n",
    "#     ## 读入clickTime_hour\n",
    "#     dt['clickTime_hour'] = dt['clickTime'].apply(dealTime)\n",
    "#     ## 读入if_appinstall\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\if_appinstall.csv'), on='userID')\n",
    "#     ## 读入if_appaction\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\if_appaction.csv'), on='userID')\n",
    "#     ## 读入count_appinstall\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\count_appinstall.csv'), on='userID')\n",
    "#     ## 读入count_appaction\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\count_appaction.csv'), on='userID')\n",
    "#     ## 读入count_user_ad\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\count_user_ad.csv'), on='userID')\n",
    "#     ## 读入vote\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\user_single_vote_simple1008_r2.csv'), on=['userID','appID'], how='left')\n",
    "#     ## 读入lastNCVR_advertiserID\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_advertiserID.csv'), on=['date', 'creativeID'])\n",
    "#     ## 读入lastNCVR_appID\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_appID.csv'), on=['date', 'creativeID'])\n",
    "#     ## 读入lastNCVR_camgaignID\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_camgaignID.csv'), on=['date', 'creativeID'])\n",
    "#     ## 读入lastNCVR_positionType\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_positionType.csv'), on=['date', 'positionID'])\n",
    "#     ## 读入lastNCVR_sitesetID\n",
    "#     dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_sitesetID.csv'), on=['date', 'positionID'])\n",
    "#     dt = dt.fillna(0)\n",
    "#     if i == 0:\n",
    "#         trainData = dt\n",
    "#     elif i == 1:\n",
    "#         testData = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 连续特征标准化\n",
    "# import sklearn.preprocessing as preprocessing\n",
    "# # features = ['lastNCVR_appID', 'lastNCVR_advertiserID','lastNCVR_positionType', 'lastNCVR_sitesetID','lastNCVR_camgaignID',\n",
    "# #            'count_appinstall', 'count_appaction','userAdCount']\n",
    "# features = ['lastNCVR_appID', 'lastNCVR_advertiserID','lastNCVR_positionType', 'lastNCVR_sitesetID','lastNCVR_camgaignID',\n",
    "#            'count_appinstall', 'count_appaction','voteMinus']\n",
    "# for i,feature in enumerate(features):\n",
    "#     # 训练集中，保存scale\n",
    "# #     scaler = preprocessing.StandardScaler()\n",
    "# #     scaler.fit(trainData[feature].values.reshape(-1,1))\n",
    "# #     joblib.dump(scaler, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\scaler' + feature + '.pkl', compress=3)\n",
    "#     # 测试集中，读取scale\n",
    "#     scaler = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\scaler' + feature + '.pkl')\n",
    "#     # 线上测试时，将最后一天的cvr提高\n",
    "#     s = 1\n",
    "# #     if feature == 'lastNCVR_appID': #1天\n",
    "# #         s = 1.34971374\n",
    "# #     elif feature == 'lastNCVR_advertiserID': #1天\n",
    "# #         s = 1.34971374\n",
    "# #     elif feature == 'lastNCVR_camgaignID': #1天\n",
    "# #         s = 1.34971374\n",
    "# #     elif feature == 'lastNCVR_sitesetID': #2天\n",
    "# #         s = 1.173481173\n",
    "# #     elif feature == 'lastNCVR_positionType': #4天\n",
    "# #         s = 1.096835297\n",
    "#     x_train = (trainData[feature]*s).values.reshape(-1,1)\n",
    "#     x_test = (testData[feature]*s).values.reshape(-1,1)\n",
    "# #     x_train = scaler.transform((trainData[feature]*s).values.reshape(-1,1))\n",
    "# #     x_test = scaler.transform((testData[feature]*s).values.reshape(-1,1))\n",
    "#     if i == 0:\n",
    "#         X_train, X_test = x_train, x_test\n",
    "#     else:\n",
    "#         X_train, X_test = np.hstack((X_train, x_train)), np.hstack((X_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 不onehot的离散变量\n",
    "# # features = ['creativeID', 'adID', 'camgaignID', 'advertiserID']\n",
    "# features = ['connectionType', 'telecomsOperator', 'marriageStatus','haveBaby', 'appPlatform', \n",
    "#             'education', 'sitesetID', 'appCategory', 'clickTime_hour','appID','age','positionType',\n",
    "#             'advertiserID', 'adID','if_appinstall','if_appaction']\n",
    "# for feature in features:\n",
    "#     x_train = trainData[feature].values.reshape(-1,1)\n",
    "#     x_test = testData[feature].values.reshape(-1,1)\n",
    "#     X_train, X_test = np.hstack((X_train, x_train)), np.hstack((X_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散变量onehot化\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "# features = ['connectionType', 'telecomsOperator', 'marriageStatus','haveBaby', 'appPlatform', \n",
    "#             'education', 'sitesetID', 'appCategory', 'clickTime_hour','appID','age','gender','positionType',\n",
    "#             'if_appinstall','if_appaction']\n",
    "#             'creativeID', 'adID', 'camgaignID', 'advertiserID', 'positionID']\n",
    "features = [\"creativeID\", \"adID\", \"camgaignID\", \"advertiserID\", \"appID\", \"appPlatform\"]\n",
    "for i,feature in enumerate(features):\n",
    "    # 训练集中，保存enc\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(trainData[feature].values.reshape(-1,1))\n",
    "    joblib.dump(enc, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\enc' + feature + '.pkl', compress=3)\n",
    "    # 测试集中，读取enc\n",
    "#     enc = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\enc' + feature + '.pkl')\n",
    "    x_train = enc.transform(trainData[feature].values.reshape(-1, 1))\n",
    "    x_test = enc.transform(testData[feature].values.reshape(-1, 1))\n",
    "    if i == 0:\n",
    "        X_train, X_test = x_train, x_test\n",
    "    else:\n",
    "        X_train, X_test = sparse.hstack((X_train, x_train)), sparse.hstack((X_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y值\n",
    "y_train = trainData['label'].values\n",
    "y_test = testData['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练GBDT树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.本地训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# # # 将训练集切分为两部分，一部分用于训练GBDT模型，另一部分输入到训练好的GBDT模型生成GBDT特征，然后作为LR的特征。这样分成两部分是为了防止过拟合。\n",
    "# X_train_gbdt, X_train_lr, y_train_gbdt, y_train_lr = cross_validation.train_test_split(X_train, y_train, test_size=0.5)\n",
    "# # # 调用GBDT分类模型。\n",
    "# grd = GradientBoostingClassifier(n_estimators=30,learning_rate=0.1, max_depth=8,max_features='sqrt',subsample=0.8)\n",
    "# # # 使用X_train训练GBDT模型，后面用此模型构造特征\n",
    "# grd.fit(X_train_gbdt, y_train_gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=1e-06, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用LR分类模型\n",
    "grd_lm = linear_model.LogisticRegressionCV(Cs=10, penalty='l2', tol=1e-6, n_jobs=-1, cv=5)\n",
    "# grd_enc = OneHotEncoder() #one-hot编码 \n",
    "# grd_enc.fit(grd.apply(X_train)[:, :, 0]) #每个数据，在每棵树，叶节点index\n",
    "# 使用训练好的GBDT模型构建特征，然后将特征经过one-hot编码作为新的特征输入到LR模型训练。\n",
    "# grd_lm.fit(grd_enc.transform(grd.apply(X_train)[:, :, 0]), y_train_lr)\n",
    "grd_lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P&R\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99    271291\n",
      "          1       0.00      0.00      0.00      7343\n",
      "\n",
      "avg / total       0.95      0.97      0.96    278634\n",
      "\n",
      "loss:\n",
      "0.116579080531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\work\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 评估模型结果\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "from sklearn import metrics\n",
    "# predict = grd_lm.predict(grd_enc.transform(grd.apply(X_test)[:, :, 0]))\n",
    "predict = grd_lm.predict(X_test)\n",
    "print 'P&R'\n",
    "print metrics.classification_report(y_test, predict)\n",
    "print 'loss:'\n",
    "# predict_pro = grd_lm.predict_proba(grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "predict_pro = grd_lm.predict_proba(X_test)[:,1]\n",
    "print logloss(y_test, predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存分类器\n",
    "# joblib.dump(grd_enc, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt_onehot_baseline.pkl', compress=3)\n",
    "# joblib.dump(grd, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt_5.pkl', compress=3)\n",
    "joblib.dump(grd_lm, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt_lr_baseline.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# # 用训练好的LR模型多X_test做预测\n",
    "# y_pred_grd_lm = grd_lm.predict_proba(grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "# # 根据预测结果输出\n",
    "# fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(fpr_grd_lm)), fpr_grd_lm)\n",
    "plt.plot(range(len(tpr_grd_lm)), tpr_grd_lm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print y_test[:10]\n",
    "print predict_pro[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pro.sum()/len(predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cf = pd.DataFrame({'columns':data.columns[1:], 'importance':list(grd.feature_importances_.T)})\n",
    "cf = pd.DataFrame({'importance':list(grd.feature_importances_.T)})\n",
    "cf.sort_values(by=['importance'],ascending=False, inplace=True)\n",
    "print cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\coef_6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取分类器\n",
    "# grd_enc = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt_onehot_baseline.pkl')\n",
    "# grd = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt_5.pkl')\n",
    "clf = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt_lr_baseline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用训练数据大概评估下模型结果\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "from sklearn import metrics\n",
    "# predict = clf.predict(grd_enc.transform(grd.apply(X_train)[:, :, 0]))\n",
    "predict = clf.predict(X_train)\n",
    "print 'P&R'\n",
    "print metrics.classification_report(y_train, predict)\n",
    "print 'loss:'\n",
    "# predict_pro = clf.predict_proba(grd_enc.transform(grd.apply(X_train)[:, :, 0]))[:, 1]\n",
    "predict_pro = clf.predict_proba(X_train)[:,1]\n",
    "print logloss(y_train, predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出预测结果\n",
    "predict_pro = clf.predict_proba(X_test)[:,1]\n",
    "# predict_pro = clf.predict_proba(grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print clf.predict_proba(grd_enc.transform(grd.apply(X_test[:5])[:, :, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pro.sum()/len(predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'instanceID': testData['instanceID'].values,'prob':predict_pro})\n",
    "output = output.sort_values(['instanceID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\gbdt_lr5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\gbdt_lr5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['prob'].mean()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
