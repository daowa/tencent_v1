{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 读入基础数据\n",
    "## 确定是读入训练集进行训练，还是读入测试集生成提交结果\n",
    "### 仅训练集\n",
    "rawData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "### 训练集和测试集。注意这里把训练集也读入了，为了dummy的时候划分一致。后续需要删除训练集数据\n",
    "# rawData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "# rawData.drop(['conversionTime'],axis=1,inplace=True) #先去除conversionTime\n",
    "# rawData.insert(0, 'instanceID', pd.Series(np.zeros(rawData['label'].count())))#插入instanceID使append不会乱序\n",
    "# rawData = pd.concat([rawData,pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\test.csv')])\n",
    "## 将部分数据读入\n",
    "rawData['date'] = rawData['clickTime'].apply(lambda x: int(x/10000))\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\user.csv'), on='userID')\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\ad.csv'), on='creativeID')\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\position.csv'), on='positionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData[rawData['label']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对点击时间进行处理\n",
    "def dealTime(clicktime):\n",
    "    hour = int(str(clicktime)[2:4])\n",
    "    minute = int(str(clicktime)[4:6])\n",
    "    x = int(60*hour+minute)/30\n",
    "    if x in [18,19,20]:\n",
    "        return 1\n",
    "    elif x in [21,22,23]:\n",
    "        return 2\n",
    "    elif x in [4,24]:\n",
    "        return 3\n",
    "    elif x in [25,26]:\n",
    "        return 4\n",
    "    elif x in [2,3,6,10,27,31,47]:\n",
    "        return 5\n",
    "    elif x in [0,1,5,7,8,9,11,28,30,44,45]:\n",
    "        return 6\n",
    "    elif x in [13,14]:\n",
    "        return 8\n",
    "    elif x in [15]:\n",
    "        return 9\n",
    "    else: #中下的最多，所以else来处理\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读入更多特征\n",
    "## 读入user_single_vote\n",
    "# rawData = pd.merge(rawData, \n",
    "#                    pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\user_single_vote.csv')[['userID','appID','voteMinus']].drop_duplicates(), \n",
    "#                    on=['userID', 'appID'])\n",
    "## 读入age\n",
    "rawData.drop(['age'], axis=1, inplace=True) #有些原有字段中有的但经过处理的，先删除，再读入\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\age.csv'), on='userID')\n",
    "## 读入appCategory\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\appCategory.csv'), on='creativeID')\n",
    "## 读入clickTime_hour\n",
    "rawData['clickTime_hour'] = rawData['clickTime'].apply(dealTime)\n",
    "## 读入lastNCVR_advertiserID\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_advertiserID.csv'), on=['date', 'creativeID'])\n",
    "## 读入lastNCVR_appID\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_appID.csv'), on=['date', 'creativeID'])\n",
    "## 读入lastNCVR_positionType\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_positionType.csv'), on=['date', 'positionID'])\n",
    "## 读入lastNCVR_sitesetID\n",
    "rawData = pd.merge(rawData, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_sitesetID.csv'), on=['date', 'positionID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 将user_single_vote离散化\n",
    "# # 获得targetID名单\n",
    "# dataAppID = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\ad.csv')\n",
    "# appIDs = set(dataAppID['appID'])\n",
    "# # 为每个appID单独生成字段\n",
    "# for id in appIDs:\n",
    "#     rawData['vm' + str(id) + '_1'] = (rawData['appID']==id).astype(int) * rawData['voteMinus']\n",
    "#     rawData['vm' + str(id) + '_0'] = (rawData['voteMinus']==0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # 特征哑变量化与离散化\n",
    "import sklearn.preprocessing as preprocessing\n",
    "## 离散变量哑变量化\n",
    "def featureDummy(features):\n",
    "    for feature in features:\n",
    "        global rawData #声明data是global而不是局部变量\n",
    "        dummies = pd.get_dummies(rawData[feature], prefix=feature)\n",
    "        rawData = pd.concat([rawData, dummies], axis=1)\n",
    "        rawData.drop([feature], axis=1, inplace=True)\n",
    "## 连续变量标准化\n",
    "dataScaleTrain = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv') #用train的数据来求scale\n",
    "dataScaleTrain['date'] = dataScaleTrain['clickTime'].apply(lambda x: int(x/10000))\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_advertiserID.csv'), on=['date', 'creativeID'])\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_appID.csv'), on=['date', 'creativeID'])\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_positionType.csv'), on=['date', 'positionID'])\n",
    "dataScaleTrain = pd.merge(dataScaleTrain, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_sitesetID.csv'), on=['date', 'positionID'])\n",
    "def featureScale(features):\n",
    "    for feature in features:\n",
    "        global rawData #声明data是global而不是局部变量\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaleParam = scaler.fit(dataScaleTrain[feature])\n",
    "        rawData[feature] = scaler.fit_transform(rawData[feature], scaleParam)\n",
    "\n",
    "# 哑变量化与离散化\n",
    "featureDummy(['connectionType', 'telecomsOperator', 'gender','marriageStatus','haveBaby', 'appPlatform', \n",
    "              'education', 'sitesetID','positionType','appCategory', 'age', 'clickTime_hour'])\n",
    "featureScale(['lastNCVR_appID', 'lastNCVR_advertiserID','lastNCVR_positionType', 'lastNCVR_sitesetID'])\n",
    "# 删除不再使用的数据，节省内存\n",
    "del dataScaleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 提取特征\n",
    "## 筛掉一些字段\n",
    "data = rawData.drop(['clickTime', 'conversionTime', 'creativeID', 'userID', 'positionID', \n",
    "                     'date', 'hometown', 'residence','advertiserID', 'adID', 'camgaignID','appID'], axis=1)\n",
    "# data = rawData.drop(['clickTime', 'creativeID', 'userID', 'positionID', \n",
    "#                      'date', 'hometown', 'residence','advertiserID', 'adID', 'camgaignID','appID'], axis=1)\n",
    "# data = rawData.filter(regex='label|vm+')\n",
    "# data = rawData.filter(regex='instanceID|label|voteMinus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 平衡正反例\n",
    "def balance(train_pro):\n",
    "    ## 从正例中随机取出比例为a的数据，划分\n",
    "    train_size_p = int(data[data['label']==1]['label'].count()*train_pro)\n",
    "    train_size_n = int(train_size_p * 39.20424181) #原始样本中负例是正例的这个倍数\n",
    "    sampler1 = np.random.permutation(data[data['label']==1]['label'].count())\n",
    "    index1 = data[data['label']==1].index[sampler1] #重排序后的索引\n",
    "    train1 = data[data['label']==1].ix[index1[:train_size_p]]\n",
    "    test1 = data[data['label']==1].ix[index1[train_size_p:]]\n",
    "    ## 从负例中随机取出相同数量的数据，划分\n",
    "    sampler0 = np.random.permutation(data[data['label']==0]['label'].count())\n",
    "    index0 = data[data['label']==0].index[sampler0] #重排序后的索引\n",
    "    train0 = data[data['label']==0].ix[index0[:train_size_n]]\n",
    "    test0 = data[data['label']==0].ix[index0[train_size_n:]]\n",
    "    ## 构建训练集和测试集\n",
    "    data_train = pd.concat([train1, train0])\n",
    "    data_test = pd.concat([test1, test0])\n",
    "    return data_train.as_matrix()[:,1:], data_test.as_matrix()[:,1:], data_train.as_matrix()[:,0],  data_test.as_matrix()[:,0]\n",
    "# 获取正反例\n",
    "trainX, testX, trainY, testY = balance(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "# X = data.as_matrix()[:,1:]\n",
    "# y = data.as_matrix()[:,0]\n",
    "# trainX, testX, trainY, testY = cross_validation.train_test_split(X, y, test_size=0.2, random_state=174)\n",
    "# y\n",
    "# 训练\n",
    "clf = linear_model.LogisticRegressionCV(Cs=10, penalty='l2', tol=1e-4, n_jobs=-1, cv=5)\n",
    "# clf = RandomForestClassifier()\n",
    "clf.fit(trainX, trainY)\n",
    "print clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 评估模型结果\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "from sklearn import metrics\n",
    "predict = clf.predict(testX)\n",
    "print 'P&R'\n",
    "print metrics.classification_report(testY, predict)\n",
    "print 'loss:'\n",
    "predict_pro = clf.predict_proba(testX)[:, 1]\n",
    "print logloss(testY, predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.DataFrame({'columns':data.columns[1:], 'coef':list(clf.coef_.T)})\n",
    "cf.sort_values(by=['coef'],ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print cf.head()\n",
    "# print cf.tail()\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\coef_4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存分类器\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\4.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取分类器\n",
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 提取特征并预测\n",
    "## 只选择instanceID>0的记录，并重排序instanceID\n",
    "dataPredict = data[data.instanceID>0].sort_values(by='instanceID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataPredict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataPredict.as_matrix()[:,2:]\n",
    "y = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'instanceID':range(1,dataPredict['instanceID'].count()+1),'prob':y[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
