{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 读入训练和测试数据\n",
    "# ## 本地，同时平衡正反例\n",
    "# def balance(train_pro):\n",
    "#     ## 从正例中随机取出比例为a的数据，划分\n",
    "#     data = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "#     train_size_p = int(data[data['label']==1]['label'].count()*train_pro)\n",
    "#     train_size_n = int(train_size_p * 39.20424181) #原始样本中负例是正例的这个倍数(所有样本)\n",
    "# #     train_size_n = int(train_size_p * 38.16572737) #原始样本中负例是正例的这个倍数（剔除了29和30号的样本）\n",
    "#     sampler1 = np.random.permutation(data[data['label']==1]['label'].count())\n",
    "#     index1 = data[data['label']==1].index[sampler1] #重排序后的索引\n",
    "#     train1 = data[data['label']==1].ix[index1[:train_size_p]]\n",
    "#     test1 = data[data['label']==1].ix[index1[train_size_p:]]\n",
    "#     ## 从负例中随机取出相同数量的数据，划分\n",
    "#     sampler0 = np.random.permutation(data[data['label']==0]['label'].count())\n",
    "#     index0 = data[data['label']==0].index[sampler0] #重排序后的索引\n",
    "#     train0 = data[data['label']==0].ix[index0[:train_size_n]]\n",
    "#     test0 = data[data['label']==0].ix[index0[train_size_n:]]\n",
    "#     ## 构建训练集和测试集\n",
    "#     data_train = pd.concat([train1, train0])\n",
    "#     data_test = pd.concat([test1, test0])\n",
    "#     return data_train, data_test\n",
    "# # 获取正反例\n",
    "# trainData, testData = balance(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "trainData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\train.csv')\n",
    "testData = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入基础数据\n",
    "for i,dt in enumerate([trainData,testData]):\n",
    "    dt['date'] = dt['clickTime'].apply(lambda x: int(x/10000))\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\user.csv'), on='userID')\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\ad.csv'), on='creativeID')\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\pre\\\\position.csv'), on='positionID')\n",
    "    if i == 0:\n",
    "        trainData = dt\n",
    "    elif i == 1:\n",
    "        testData = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地测试与训练\n",
    "testData = trainData[trainData['date']==28]\n",
    "trainData = trainData[trainData['date']<28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对点击时间进行处理\n",
    "def dealTime(clicktime):\n",
    "    hour = int(str(clicktime)[2:4])\n",
    "    minute = int(str(clicktime)[4:6])\n",
    "    x = int(60*hour+minute)/30\n",
    "    if x in [18,19,20]:\n",
    "        return 1\n",
    "    elif x in [21,22,23]:\n",
    "        return 2\n",
    "    elif x in [4,24]:\n",
    "        return 3\n",
    "    elif x in [25,26]:\n",
    "        return 4\n",
    "    elif x in [2,3,6,10,27,31,47]:\n",
    "        return 5\n",
    "    elif x in [0,1,5,7,8,9,11,28,30,44,45]:\n",
    "        return 6\n",
    "    elif x in [13,14]:\n",
    "        return 8\n",
    "    elif x in [15]:\n",
    "        return 9\n",
    "    else: #中下的最多，所以else来处理\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入更多特征\n",
    "for i,dt in enumerate([trainData,testData]):\n",
    "    ## 读入age\n",
    "    dt.drop(['age'], axis=1, inplace=True) #有些原有字段中有的但经过处理的，先删除，再读入\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\age.csv'), on='userID')\n",
    "    ## residence提取省份\n",
    "    # dt['residence'] = dt['residence'].apply(lambda x: int(str(x)[-4:-2]) if x>0 else 0)\n",
    "    ## 读入appCategory\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\appCategory.csv'), on='creativeID')\n",
    "    ## 读入clickTime_hour\n",
    "    dt['clickTime_hour'] = dt['clickTime'].apply(dealTime)\n",
    "    ## 读入if_appinstall\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\if_appinstall.csv'), on='userID')\n",
    "    ## 读入if_appaction\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\if_appaction.csv'), on='userID')\n",
    "    ## 读入count_appinstall\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\count_appinstall.csv'), on='userID')\n",
    "    ## 读入count_appaction\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\count_appaction.csv'), on='userID')\n",
    "    ## 读入lastNCVR_advertiserID\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_advertiserID.csv'), on=['date', 'creativeID'])\n",
    "    ## 读入lastNCVR_appID\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_appID.csv'), on=['date', 'creativeID'])\n",
    "    ## 读入lastNCVR_camgaignID\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_camgaignID.csv'), on=['date', 'creativeID'])\n",
    "    ## 读入lastNCVR_positionType\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_positionType.csv'), on=['date', 'positionID'])\n",
    "    ## 读入lastNCVR_sitesetID\n",
    "    dt = pd.merge(dt, pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\feature\\\\lastNCVR_sitesetID.csv'), on=['date', 'positionID'])\n",
    "    if i == 0:\n",
    "        trainData = dt\n",
    "    elif i == 1:\n",
    "        testData = dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.读入基本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征标准化\n",
    "import sklearn.preprocessing as preprocessing\n",
    "# features = ['lastNCVR_appID', 'lastNCVR_advertiserID','lastNCVR_positionType', 'lastNCVR_sitesetID','lastNCVR_camgaignID',\n",
    "#            'count_appinstall','count_appaction']\n",
    "features= ['count_appaction']\n",
    "scaler = preprocessing.StandardScaler()\n",
    "for i,feature in enumerate(features):\n",
    "    x_train = scaler.fit_transform(trainData[feature].values.reshape(-1,1))\n",
    "    x_test = scaler.transform(testData[feature].values.reshape(-1,1))\n",
    "    if i == 0:\n",
    "        X_train, X_test = x_train, x_test\n",
    "    else:\n",
    "        X_train, X_test = np.hstack((X_train, x_train)), np.hstack((X_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散变量onehot化\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "enc = OneHotEncoder()\n",
    "features = ['connectionType', 'telecomsOperator', 'marriageStatus','haveBaby', 'appPlatform', \n",
    "            'education', 'sitesetID', 'appCategory', 'clickTime_hour','appID','age','gender','positionType',\n",
    "            'if_appinstall','if_appaction']\n",
    "#             'creativeID', 'adID', 'camgaignID', 'advertiserID', 'positionID']\n",
    "for feature in features:\n",
    "    x_train = enc.fit_transform(trainData[feature].values.reshape(-1, 1))\n",
    "    x_test = enc.transform(testData[feature].values.reshape(-1, 1))\n",
    "    X_train, X_test = sparse.hstack((X_train, x_train)), sparse.hstack((X_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.读入GBDT特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 连续特征标准化\n",
    "import sklearn.preprocessing as preprocessing\n",
    "features = ['lastNCVR_appID', 'lastNCVR_advertiserID','lastNCVR_positionType', 'lastNCVR_sitesetID','lastNCVR_camgaignID']\n",
    "scaler = preprocessing.StandardScaler()\n",
    "for i,feature in enumerate(features):\n",
    "    x_train_gbdt = scaler.fit_transform(trainData[feature].values.reshape(-1,1))\n",
    "    x_test_gbdt = scaler.fit_transform(testData[feature].values.reshape(-1,1))\n",
    "    if i == 0:\n",
    "        X_train_gbdt, X_test_gbdt = x_train_gbdt, x_test_gbdt\n",
    "    else:\n",
    "        X_train_gbdt, X_test_gbdt = np.hstack((X_train_gbdt, x_train_gbdt)), np.hstack((X_test_gbdt, x_test_gbdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不onehot的离散变量\n",
    "# features = ['creativeID', 'adID', 'camgaignID', 'advertiserID']\n",
    "features = ['connectionType', 'telecomsOperator', 'marriageStatus','haveBaby', 'appPlatform', \n",
    "            'education', 'sitesetID', 'appCategory', 'clickTime_hour','appID','age','positionType',\n",
    "            'advertiserID', 'adID','if_appinstall','if_appaction']\n",
    "for feature in features:\n",
    "    x_train_gbdt = trainData[feature].values.reshape(-1,1)\n",
    "    x_test_gbdt = testData[feature].values.reshape(-1,1)\n",
    "    X_train_gbdt, X_test_gbdt = np.hstack((X_train_gbdt, x_train_gbdt)), np.hstack((X_test_gbdt, x_test_gbdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取gbdt分类器\n",
    "from sklearn.externals import joblib\n",
    "grd = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\gbdt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得gbdt特征\n",
    "grd_enc = OneHotEncoder() #one-hot编码 \n",
    "X_train_gbdt = grd_enc.fit_transform(grd.apply(X_train_gbdt)[:, :, 0]) #每个数据，在每棵树，叶节点index\n",
    "X_test_gbdt = grd_enc.transform(grd.apply(X_test_gbdt)[:, :, 0])\n",
    "X_train = sparse.hstack((X_train, X_train_gbdt))\n",
    "X_test = sparse.hstack((X_test, X_test_gbdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.读入y值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y值\n",
    "y_train = trainData['label'].values\n",
    "y_test = testData['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "# 训练\n",
    "clf = linear_model.LogisticRegressionCV(Cs=10, penalty='l2', tol=1e-4, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P&R\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99   2762579\n",
      "          1       0.00      0.00      0.00     72149\n",
      "\n",
      "avg / total       0.95      0.97      0.96   2834728\n",
      "\n",
      "loss:\n",
      "0.117777937359\n"
     ]
    }
   ],
   "source": [
    "# 评估模型结果\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "from sklearn import metrics\n",
    "predict = clf.predict(X_test)\n",
    "print 'P&R'\n",
    "print metrics.classification_report(y_test, predict)\n",
    "print 'loss:'\n",
    "predict_pro = clf.predict_proba(X_test)[:, 1]\n",
    "# predict_pro = predict_pro - predict_pro.min()\n",
    "print logloss(y_test, predict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.02242665  0.04746538  0.02242665  0.02242665  0.02242665  0.02242665\n",
      "  0.02242665  0.02242665  0.02242665  0.02242665]\n"
     ]
    }
   ],
   "source": [
    "print y_test[10:20]\n",
    "print predict_pro[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf = pd.DataFrame({'columns':data.columns[1:], 'importance':list(grd.feature_importances_.T)})\n",
    "# cf = pd.DataFrame({'importance':list(grd.feature_importances_.T)})\n",
    "cf.sort_values(by=['importance'],ascending=False, inplace=True)\n",
    "print cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\coef_6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存分类器\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(grd_lm, 'C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\lr_0.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取分类器\n",
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\clf\\\\1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征并预测\n",
    "## 只选择instanceID>0的记录，并重排序instanceID\n",
    "dataPredict = data[data.instanceID>0].sort_values(by='instanceID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPredict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataPredict.as_matrix()[:,2:]\n",
    "y = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'instanceID':range(1,dataPredict['instanceID'].count()+1),'prob':y[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('C:\\\\Users\\\\work\\\\Desktop\\\\tencent\\\\predict\\\\5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
